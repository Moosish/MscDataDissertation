{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/statsbomb/open-data.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evZ4iw_1r_vg",
        "outputId": "ce2a887c-0423-43bc-a993-789694144993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'open-data'...\n",
            "remote: Enumerating objects: 49843, done.\u001b[K\n",
            "remote: Counting objects: 100% (5351/5351), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1332/1332), done.\u001b[K\n",
            "remote: Total 49843 (delta 5243), reused 4097 (delta 3999), pack-reused 44492 (from 1)\u001b[K\n",
            "Receiving objects: 100% (49843/49843), 6.45 GiB | 18.02 MiB/s, done.\n",
            "Resolving deltas: 100% (46913/46913), done.\n",
            "Updating files: 100% (7246/7246), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import json\n",
        "from statsbombpy import sb\n",
        "\n",
        "# Path to the directory containing the event files\n",
        "event_files_path = '/content/open-data/data/events'\n",
        "\n",
        "# Function to load and process each event file\n",
        "def process_event_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        events = json.load(f)\n",
        "    return pd.json_normalize(events)\n",
        "\n",
        "# Sample a fraction of the event data (e.g., 10%)\n",
        "sampling_fraction = 0.1\n",
        "\n",
        "# Loop through all event files and process them\n",
        "all_event_data = []\n",
        "for i, file_name in enumerate(os.listdir(event_files_path)):\n",
        "    if file_name.endswith('.json'):\n",
        "        file_path = os.path.join(event_files_path, file_name)\n",
        "        event_df = process_event_file(file_path)\n",
        "\n",
        "        # Randomly sample a fraction of the data\n",
        "        sampled_df = event_df.sample(frac=sampling_fraction, random_state=42)\n",
        "        all_event_data.append(sampled_df)\n",
        "\n",
        "        # Optionally, break early if too many files are loaded\n",
        "        if i >= 20:  # Load only the first 20 files for this example\n",
        "            break\n",
        "\n",
        "# Concatenate all event data into a single DataFrame\n",
        "all_event_data_df = pd.concat(all_event_data, ignore_index=True)\n",
        "\n",
        "# Display the size of the data to ensure it fits into RAM\n",
        "print(f\"Total events loaded: {len(all_event_data_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WleZBBBTsASB",
        "outputId": "7b4c415e-21d9-47e1-aba1-44a7ccc9900a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total events loaded: 7677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_features(event_df):\n",
        "    # Extract key match context features\n",
        "    event_df['minute'] = event_df['minute']\n",
        "\n",
        "    # Add additional features as needed (e.g., counts of events, etc.)\n",
        "    player_actions = event_df.groupby('player.id').agg({\n",
        "        'pass.outcome.name': 'count', # Changed from 'sum' to 'count' to avoid string concatenation\n",
        "        'shot.outcome.name': 'count', # Changed from 'sum' to 'count' to avoid string concatenation\n",
        "        'foul_committed.card.name': 'count' # Changed from 'sum' to 'count' to avoid string concatenation\n",
        "    }).reset_index()\n",
        "\n",
        "    # Rename the columns to be more descriptive\n",
        "    player_actions.rename(columns={\n",
        "        'pass.outcome.name': 'total_passes',\n",
        "        'shot.outcome.name': 'total_shots',\n",
        "        'foul_committed.card.name': 'total_fouls'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Merge player actions back into the main DataFrame using a left merge to preserve 'event_type'\n",
        "    event_df = event_df.merge(player_actions, on='player.id', how='left')\n",
        "\n",
        "    # Convert columns to numeric, replacing non-numeric values with 0\n",
        "    for col in ['total_passes', 'total_shots', 'total_fouls']:\n",
        "        event_df[col] = pd.to_numeric(event_df[col], errors='coerce').fillna(0)\n",
        "\n",
        "    return event_df\n",
        "\n",
        "# Apply create_features to the DataFrame\n",
        "all_event_data_df = create_features(all_event_data_df)"
      ],
      "metadata": {
        "id": "8LG89cMgsALO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare features and labels\n",
        "features = all_event_data_df[['minute', 'total_passes', 'total_shots', 'total_fouls']]  # Add more features as needed\n",
        "labels = all_event_data_df['type.name'].apply(lambda x: 1 if x == 'Substitution' else 0)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "from sklearn.metrics import accuracy_score\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Model Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvGVE1MYsEqV",
        "outputId": "86b05ec2-3b7b-457f-8745-cfbaa9380663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 0.9973958333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained model to a file\n",
        "joblib.dump(model, 'substitution_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFQOXhV36iAn",
        "outputId": "d79ac079-a788-4819-994d-9c3971b98ce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['substitution_model.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flask-cors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBeafJQ3YuIH",
        "outputId": "269965a4-cc80-4209-8f48-99b7f6c5581a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: Flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.0.4)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (3.1.4)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.9->flask-cors) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask>=0.9->flask-cors) (2.1.5)\n",
            "Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
            "Installing collected packages: flask-cors\n",
            "Successfully installed flask-cors-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from flask import Flask, request, jsonify\n",
        "import joblib\n",
        "import numpy as np\n",
        "from flask_cors import CORS\n",
        "import logging\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Load the trained model\n",
        "try:\n",
        "    model = joblib.load('substitution_model.pkl')\n",
        "    logging.info(\"Model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    logging.error(f\"Error loading model: {e}\")\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)  # Enable CORS for all routes\n",
        "\n",
        "@app.route('/predict', methods=['POST'])\n",
        "def predict():\n",
        "    if request.method == 'OPTIONS':\n",
        "        response = jsonify({})\n",
        "        response.headers.add(\"Access-Control-Allow-Origin\", \"*\")\n",
        "        response.headers.add(\"Access-Control-Allow-Headers\", \"Content-Type\")\n",
        "        response.headers.add(\"Access-Control-Allow-Methods\", \"POST, OPTIONS\")\n",
        "        return response\n",
        "    try:\n",
        "        data = request.get_json()\n",
        "        current_events = data.get(\"events\")\n",
        "\n",
        "        # Preprocess the events (this should match your training process)\n",
        "        X = preprocess_events(current_events)\n",
        "\n",
        "        # Log the shape of the preprocessed data to verify the feature count\n",
        "        logging.info(f\"Shape of preprocessed data: {X.shape}\")\n",
        "\n",
        "        # Get prediction probabilities\n",
        "        proba = model.predict_proba(X)\n",
        "\n",
        "        # Assuming two teams, return probabilities for substitutions for each team\n",
        "        result = {\n",
        "            \"team_1_substitution_probability\": proba[0][1],  # Probability for the first team\n",
        "            \"team_2_substitution_probability\": proba[1][1]   # Probability for the second team\n",
        "        }\n",
        "\n",
        "        return jsonify(result)\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error during prediction: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "def preprocess_events(events):\n",
        "    # Convert the event data into the format your model expects\n",
        "    try:\n",
        "        # Initialize a list for storing features\n",
        "        features = []\n",
        "\n",
        "        for event in events:\n",
        "            # Extract relevant features that match the model's training process\n",
        "            # Adjust this to match exactly the features your model was trained on\n",
        "\n",
        "            event_type = event['type'].get('name', 'Unknown')\n",
        "            minute = event.get('minute', 0)\n",
        "            second = event.get('second', 0)\n",
        "            team = event['team'].get('name', 'Unknown')\n",
        "\n",
        "            # Assuming that only 4 features were used in training, select only 4\n",
        "            feature_vector = [\n",
        "                minute,             # Numeric feature\n",
        "                second,             # Numeric feature\n",
        "                len(event_type),    # Example of a categorical feature converted to numeric\n",
        "                len(team)           # Another categorical feature converted to numeric\n",
        "            ]\n",
        "\n",
        "            # Append the feature vector to the list\n",
        "            features.append(feature_vector)\n",
        "\n",
        "        # Convert the list to a numpy array for model input\n",
        "        X = np.array(features)\n",
        "\n",
        "        # Log the feature vectors for debugging\n",
        "        logging.debug(f\"Feature vectors: {X}\")\n",
        "\n",
        "        return X\n",
        "\n",
        "    except KeyError as e:\n",
        "        logging.error(f\"Missing expected feature: {e}\")\n",
        "        raise e\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in preprocessing: {e}\")\n",
        "        raise e\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=8080, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QThOoKKV6i03",
        "outputId": "2d894c71-6418-4aae-cd4a-16f0d7985abd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "# Use an official Python runtime as a parent image\n",
        "FROM python:3.8-slim\n",
        "\n",
        "# Set the working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the current directory contents into the container at /app\n",
        "COPY . /app\n",
        "\n",
        "# Install any necessary dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Make port 8080 available to the world outside this container\n",
        "EXPOSE 8080\n",
        "\n",
        "# Define environment variable\n",
        "ENV NAME World\n",
        "\n",
        "# Run app.py when the container launches\n",
        "CMD [\"python\", \"app.py\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93YGGk4J6od5",
        "outputId": "e0e8e332-bddd-47fa-b293-51b9bc06eb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile requirements.txt\n",
        "Flask\n",
        "joblib\n",
        "numpy\n",
        "scikit-learn\n",
        "flask-cors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX0sAvr06sB4",
        "outputId": "ef17cf08-10aa-4922-d211-845d72e508ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile .dockerignore\n",
        "# Ignore Python cache files\n",
        "__pycache__/\n",
        "*.pyc\n",
        "\n",
        "# Ignore specific directories\n",
        "sample_data\n",
        "open-data\n",
        "\n",
        "# Ignore any temporary files or directories\n",
        "*.tmp\n",
        "*.log\n",
        "temp/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW_WNzth7iG1",
        "outputId": "cb862d9c-0662-4af1-f5e0-2d3a8adc246c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting .dockerignore\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmNyjoEA60ar",
        "outputId": "b87c88ba-18b9-4f94-be6e-edd1ee367770"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=QdgxOTGQqE4AUtZ3tjYNJO17JzfGdF&prompt=consent&token_usage=remote&access_type=offline&code_challenge=qhnaPIEIyCyqAs2olsBV9EXgWsGD5IaNhXlHbxq0Ml0&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AQlEd8xKKk9o3lK0Mc27j9vEZee9i65HRgH3c-KbRHuvb-cLrioRW-L07yR-jVs5lY_2sw\n",
            "\n",
            "You are now logged in as [thomasgeorgepasley@gmail.com].\n",
            "Your current project is [None].  You can change this setting by running:\n",
            "  $ gcloud config set project PROJECT_ID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project polar-ensign-432610-t7"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzj5lI1d63DZ",
        "outputId": "ae6ea13f-7786-4aca-ddbe-60351818b5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/app"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46aDgZZi8Q9S",
        "outputId": "5cfb2ef8-8d01-4c66-9f93-fe43efee1259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wbG6LVz8S6Q",
        "outputId": "884b16a9-11ce-46f7-ddb3-21a524ebcb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\tDockerfile  requirements.txt  substitution_model.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud builds submit --tag gcr.io/polar-ensign-432610-t7/substitution-model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqNCzQbd7Kbq",
        "outputId": "e55b34f5-a71f-49dd-c1cc-3bb28fe699a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating temporary archive of 4 file(s) totalling 352.4 KiB before compression.\n",
            "Uploading tarball of [.] to [gs://polar-ensign-432610-t7_cloudbuild/source/1724856894.389428-08cc073ad7fd4e1288338d42793a56d3.tgz]\n",
            "Created [https://cloudbuild.googleapis.com/v1/projects/polar-ensign-432610-t7/locations/global/builds/65bce125-93d4-4ce1-8d27-08c6aabb7887].\n",
            "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/65bce125-93d4-4ce1-8d27-08c6aabb7887?project=116084333061 ].\n",
            "Waiting for build to complete. Polling interval: 1 second(s).\n",
            " REMOTE BUILD OUTPUT\n",
            "starting build \"65bce125-93d4-4ce1-8d27-08c6aabb7887\"\n",
            "\n",
            "FETCHSOURCE\n",
            "Fetching storage object: gs://polar-ensign-432610-t7_cloudbuild/source/1724856894.389428-08cc073ad7fd4e1288338d42793a56d3.tgz#1724856895278207\n",
            "Copying gs://polar-ensign-432610-t7_cloudbuild/source/1724856894.389428-08cc073ad7fd4e1288338d42793a56d3.tgz#1724856895278207...\n",
            "/ [1 files][ 62.3 KiB/ 62.3 KiB]                                                \n",
            "Operation completed over 1 objects/62.3 KiB.\n",
            "BUILD\n",
            "Already have image (with digest): gcr.io/cloud-builders/docker\n",
            "Sending build context to Docker daemon  365.6kB\n",
            "Step 1/7 : FROM python:3.8-slim\n",
            "3.8-slim: Pulling from library/python\n",
            "e4fff0779e6d: Already exists\n",
            "4a5c74102edc: Pulling fs layer\n",
            "8d7f4eef7e05: Pulling fs layer\n",
            "120a794db9c9: Pulling fs layer\n",
            "188f8c4ef238: Pulling fs layer\n",
            "188f8c4ef238: Waiting\n",
            "120a794db9c9: Verifying Checksum\n",
            "120a794db9c9: Download complete\n",
            "4a5c74102edc: Verifying Checksum\n",
            "4a5c74102edc: Download complete\n",
            "8d7f4eef7e05: Verifying Checksum\n",
            "8d7f4eef7e05: Download complete\n",
            "4a5c74102edc: Pull complete\n",
            "188f8c4ef238: Download complete\n",
            "8d7f4eef7e05: Pull complete\n",
            "120a794db9c9: Pull complete\n",
            "188f8c4ef238: Pull complete\n",
            "Digest: sha256:f8b4609a66cdaa133fa57e2ca8e2f03de2ebb44ffefb4c0b8b2de782aefca4a1\n",
            "Status: Downloaded newer image for python:3.8-slim\n",
            " ---> e7fd04b8ffc7\n",
            "Step 2/7 : WORKDIR /app\n",
            " ---> Running in 3b1b8571ea60\n",
            "Removing intermediate container 3b1b8571ea60\n",
            " ---> 05c0f810af47\n",
            "Step 3/7 : COPY . /app\n",
            " ---> 11cbf88bbfac\n",
            "Step 4/7 : RUN pip install --no-cache-dir -r requirements.txt\n",
            " ---> Running in e4c0e70f95f7\n",
            "Collecting Flask\n",
            "  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.7/101.7 kB 5.5 MB/s eta 0:00:00\n",
            "Collecting joblib\n",
            "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 301.8/301.8 kB 36.9 MB/s eta 0:00:00\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 188.5 MB/s eta 0:00:00\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.1/11.1 MB 177.9 MB/s eta 0:00:00\n",
            "Collecting flask-cors\n",
            "  Downloading Flask_Cors-4.0.1-py2.py3-none-any.whl (14 kB)\n",
            "Collecting click>=8.1.3\n",
            "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 174.0 MB/s eta 0:00:00\n",
            "Collecting itsdangerous>=2.1.2\n",
            "  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
            "Collecting Jinja2>=3.1.2\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.3/133.3 kB 181.9 MB/s eta 0:00:00\n",
            "Collecting blinker>=1.6.2\n",
            "  Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\n",
            "Collecting importlib-metadata>=3.6.0\n",
            "  Downloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting Werkzeug>=3.0.0\n",
            "  Downloading werkzeug-3.0.4-py3-none-any.whl (227 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.6/227.6 kB 207.3 MB/s eta 0:00:00\n",
            "Collecting scipy>=1.5.0\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.5/34.5 MB 190.4 MB/s eta 0:00:00\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.20.1-py3-none-any.whl (9.0 kB)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
            "Installing collected packages: zipp, threadpoolctl, numpy, MarkupSafe, joblib, itsdangerous, click, blinker, Werkzeug, scipy, Jinja2, importlib-metadata, scikit-learn, Flask, flask-cors\n",
            "Successfully installed Flask-3.0.3 Jinja2-3.1.4 MarkupSafe-2.1.5 Werkzeug-3.0.4 blinker-1.8.2 click-8.1.7 flask-cors-4.0.1 importlib-metadata-8.4.0 itsdangerous-2.2.0 joblib-1.4.2 numpy-1.24.4 scikit-learn-1.3.2 scipy-1.10.1 threadpoolctl-3.5.0 zipp-3.20.1\n",
            "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
            "\u001b[0m\u001b[91m\n",
            "[notice] A new release of pip is available: 23.0.1 -> 24.2\n",
            "[notice] To update, run: pip install --upgrade pip\n",
            "\u001b[0mRemoving intermediate container e4c0e70f95f7\n",
            " ---> bc7b36986685\n",
            "Step 5/7 : EXPOSE 8080\n",
            " ---> Running in c791bee0c126\n",
            "Removing intermediate container c791bee0c126\n",
            " ---> 12d90cec8464\n",
            "Step 6/7 : ENV NAME World\n",
            " ---> Running in e72bcbc442e0\n",
            "Removing intermediate container e72bcbc442e0\n",
            " ---> 473da9af61d7\n",
            "Step 7/7 : CMD [\"python\", \"app.py\"]\n",
            " ---> Running in 0be1249f64fd\n",
            "Removing intermediate container 0be1249f64fd\n",
            " ---> 1cd72948fa96\n",
            "Successfully built 1cd72948fa96\n",
            "Successfully tagged gcr.io/polar-ensign-432610-t7/substitution-model:latest\n",
            "PUSH\n",
            "Pushing gcr.io/polar-ensign-432610-t7/substitution-model\n",
            "The push refers to repository [gcr.io/polar-ensign-432610-t7/substitution-model]\n",
            "e77ce2b7cc82: Preparing\n",
            "ab8a11c3c68f: Preparing\n",
            "7b92874a032f: Preparing\n",
            "ea3ca52555d9: Preparing\n",
            "c051c8ddf0e4: Preparing\n",
            "3fd23da07c85: Preparing\n",
            "d459f4cb7e83: Preparing\n",
            "9853575bc4f9: Preparing\n",
            "d459f4cb7e83: Waiting\n",
            "9853575bc4f9: Waiting\n",
            "3fd23da07c85: Waiting\n",
            "c051c8ddf0e4: Layer already exists\n",
            "ea3ca52555d9: Layer already exists\n",
            "d459f4cb7e83: Layer already exists\n",
            "3fd23da07c85: Layer already exists\n",
            "9853575bc4f9: Layer already exists\n",
            "7b92874a032f: Pushed\n",
            "ab8a11c3c68f: Pushed\n",
            "e77ce2b7cc82: Pushed\n",
            "latest: digest: sha256:ca7520c7be655ad61549352b3dd2311d14a528be0c89de728827bd99cc1821ed size: 1998\n",
            "DONE\n",
            "\n",
            "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                                IMAGES                                                      STATUS\n",
            "65bce125-93d4-4ce1-8d27-08c6aabb7887  2024-08-28T14:54:55+00:00  50S       gs://polar-ensign-432610-t7_cloudbuild/source/1724856894.389428-08cc073ad7fd4e1288338d42793a56d3.tgz  gcr.io/polar-ensign-432610-t7/substitution-model (+1 more)  SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud run deploy substitution-model-service \\\n",
        "--image gcr.io/polar-ensign-432610-t7/substitution-model \\\n",
        "--platform managed \\\n",
        "--region eu-west2 \\\n",
        "--allow-unauthenticated\n"
      ],
      "metadata": {
        "id": "TRhFTUq5690U"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}